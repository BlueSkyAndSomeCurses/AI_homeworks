{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5f8a91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import center_of_mass, shift\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "from sklearn.svm import SVC\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "be4f3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pl.read_csv(\n",
    "    \"./data/train.csv\", schema={\"filename\": pl.String, \"label\": pl.String}\n",
    ").with_columns((\"./data/train/\" + pl.col(\"filename\")).alias(\"filename\"))\n",
    "\n",
    "test_data = pl.read_csv(\"./data/test.csv\").with_columns(\n",
    "    (\"./data/test/\" + pl.col(\"filename\")).alias(\"filename\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "17f79410",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd4c15",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Firstly, I want to see the distribution of the lables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4af39051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>filename</th><th>label</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;60000&quot;</td><td>&quot;60000&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;./data/train/0000a924c12d.png&quot;</td><td>&quot;10&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;./data/train/fffed4bad0cf.png&quot;</td><td>&quot;99&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 3)\n",
       "┌────────────┬───────────────────────────────┬───────┐\n",
       "│ statistic  ┆ filename                      ┆ label │\n",
       "│ ---        ┆ ---                           ┆ ---   │\n",
       "│ str        ┆ str                           ┆ str   │\n",
       "╞════════════╪═══════════════════════════════╪═══════╡\n",
       "│ count      ┆ 60000                         ┆ 60000 │\n",
       "│ null_count ┆ 0                             ┆ 0     │\n",
       "│ mean       ┆ null                          ┆ null  │\n",
       "│ std        ┆ null                          ┆ null  │\n",
       "│ min        ┆ ./data/train/0000a924c12d.png ┆ 10    │\n",
       "│ 25%        ┆ null                          ┆ null  │\n",
       "│ 50%        ┆ null                          ┆ null  │\n",
       "│ 75%        ┆ null                          ┆ null  │\n",
       "│ max        ┆ ./data/train/fffed4bad0cf.png ┆ 99    │\n",
       "└────────────┴───────────────────────────────┴───────┘"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3d8c69e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(train_data.select(pl.col(\"label\").cast(pl.Int32)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a56623",
   "metadata": {},
   "source": [
    "Okay, I see that dataset is seriously imabalnced, to number 40 -- 46. I have looked over images in the dataset, and I suppose they are perfectly separable in half. In future I may consider training it on the original MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bcd63",
   "metadata": {},
   "source": [
    "Supposing I have split image in 2 parts, I can already embeed it in feature vector and train MLP or SVM on it. But I notices that digits are scattared from top to bottom. In my opinion this may make it harder for model to seperate a space. As it will learn this \"spatial\" position of a digit as feature. So I further notices that all the digits are almost of the same scale. I will simply trim each image, to fit constant size. But for that I need to analyze \"heights\" of each seperate image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f4454956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "random_image = cv2.cvtColor(\n",
    "    cv2.imread(train_data[\"filename\"].sample(n=1)[0]), cv2.COLOR_BGR2GRAY\n",
    ")\n",
    "plt.imshow(random_image)\n",
    "plt.show()\n",
    "\n",
    "print(random_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8b3a694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_right_parts(image_path: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY)\n",
    "    if image is None:\n",
    "        raise ValueError(\"asd\")\n",
    "\n",
    "    w, h = image.shape\n",
    "    image_left, image_right = image[:, : h // 2], image[:, h // 2 :]\n",
    "\n",
    "    return image_left, image_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "28aee0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_last_pixel(image: np.ndarray) -> list[int]:\n",
    "    not_black_rows = np.where(np.any(image > 0, axis=1))[0]\n",
    "    not_black_cols = np.where(np.any(image > 0, axis=0))[0]\n",
    "\n",
    "    return [\n",
    "        not_black_rows[0],\n",
    "        not_black_rows[-1],\n",
    "        not_black_cols[0],\n",
    "        not_black_cols[-1],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "db40da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_set(data: pl.DataFrame) -> pl.DataFrame:\n",
    "    return data.with_columns(\n",
    "        pl.col(\"filename\")\n",
    "        .map_elements(\n",
    "            lambda path_: find_first_last_pixel(get_left_right_parts(path_)[0]),\n",
    "            return_dtype=pl.List(pl.Int32),\n",
    "        )\n",
    "        .alias(\"LeftDigitBounds\"),\n",
    "        pl.col(\"filename\")\n",
    "        .map_elements(\n",
    "            lambda path_: find_first_last_pixel(get_left_right_parts(path_)[-1]),\n",
    "            return_dtype=pl.List(pl.Int32),\n",
    "        )\n",
    "        .alias(\"RightDigitBounds\"),\n",
    "    ).with_columns(\n",
    "        (\n",
    "            pl.col(\"LeftDigitBounds\").list.get(1)\n",
    "            - pl.col(\"LeftDigitBounds\").list.get(0)\n",
    "        ).alias(\"LeftHeight\"),\n",
    "        (\n",
    "            pl.col(\"LeftDigitBounds\").list.get(3)\n",
    "            - pl.col(\"LeftDigitBounds\").list.get(2)\n",
    "        ).alias(\"LeftWidth\"),\n",
    "        (\n",
    "            pl.col(\"RightDigitBounds\").list.get(1)\n",
    "            - pl.col(\"RightDigitBounds\").list.get(0)\n",
    "        ).alias(\"RightHeight\"),\n",
    "        (\n",
    "            pl.col(\"RightDigitBounds\").list.get(3)\n",
    "            - pl.col(\"RightDigitBounds\").list.get(2)\n",
    "        ).alias(\"RightWidth\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "99a6174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_preprocessed = preprocess_data_set(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "196c448c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>filename</th><th>label</th><th>LeftDigitBounds</th><th>RightDigitBounds</th><th>LeftHeight</th><th>LeftWidth</th><th>RightHeight</th><th>RightWidth</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;60000&quot;</td><td>&quot;60000&quot;</td><td>60000.0</td><td>60000.0</td><td>60000.0</td><td>60000.0</td><td>60000.0</td><td>60000.0</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>18.760567</td><td>15.118883</td><td>18.743467</td><td>15.3106</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.941428</td><td>3.411355</td><td>0.95347</td><td>3.363329</td></tr><tr><td>&quot;min&quot;</td><td>&quot;./data/train/0000a924c12d.png&quot;</td><td>&quot;10&quot;</td><td>null</td><td>null</td><td>8.0</td><td>2.0</td><td>9.0</td><td>1.0</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>19.0</td><td>13.0</td><td>19.0</td><td>13.0</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>19.0</td><td>15.0</td><td>19.0</td><td>15.0</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>19.0</td><td>19.0</td><td>19.0</td><td>19.0</td></tr><tr><td>&quot;max&quot;</td><td>&quot;./data/train/fffed4bad0cf.png&quot;</td><td>&quot;99&quot;</td><td>null</td><td>null</td><td>19.0</td><td>19.0</td><td>19.0</td><td>19.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 9)\n",
       "┌────────────┬────────────┬───────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ statistic  ┆ filename   ┆ label ┆ LeftDigitB ┆ … ┆ LeftHeigh ┆ LeftWidth ┆ RightHeig ┆ RightWidt │\n",
       "│ ---        ┆ ---        ┆ ---   ┆ ounds      ┆   ┆ t         ┆ ---       ┆ ht        ┆ h         │\n",
       "│ str        ┆ str        ┆ str   ┆ ---        ┆   ┆ ---       ┆ f64       ┆ ---       ┆ ---       │\n",
       "│            ┆            ┆       ┆ f64        ┆   ┆ f64       ┆           ┆ f64       ┆ f64       │\n",
       "╞════════════╪════════════╪═══════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ count      ┆ 60000      ┆ 60000 ┆ 60000.0    ┆ … ┆ 60000.0   ┆ 60000.0   ┆ 60000.0   ┆ 60000.0   │\n",
       "│ null_count ┆ 0          ┆ 0     ┆ 0.0        ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
       "│ mean       ┆ null       ┆ null  ┆ null       ┆ … ┆ 18.760567 ┆ 15.118883 ┆ 18.743467 ┆ 15.3106   │\n",
       "│ std        ┆ null       ┆ null  ┆ null       ┆ … ┆ 0.941428  ┆ 3.411355  ┆ 0.95347   ┆ 3.363329  │\n",
       "│ min        ┆ ./data/tra ┆ 10    ┆ null       ┆ … ┆ 8.0       ┆ 2.0       ┆ 9.0       ┆ 1.0       │\n",
       "│            ┆ in/0000a92 ┆       ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 4c12d.png  ┆       ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 25%        ┆ null       ┆ null  ┆ null       ┆ … ┆ 19.0      ┆ 13.0      ┆ 19.0      ┆ 13.0      │\n",
       "│ 50%        ┆ null       ┆ null  ┆ null       ┆ … ┆ 19.0      ┆ 15.0      ┆ 19.0      ┆ 15.0      │\n",
       "│ 75%        ┆ null       ┆ null  ┆ null       ┆ … ┆ 19.0      ┆ 19.0      ┆ 19.0      ┆ 19.0      │\n",
       "│ max        ┆ ./data/tra ┆ 99    ┆ null       ┆ … ┆ 19.0      ┆ 19.0      ┆ 19.0      ┆ 19.0      │\n",
       "│            ┆ in/fffed4b ┆       ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ ad0cf.png  ┆       ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴────────────┴───────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_preprocessed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a9ac5",
   "metadata": {},
   "source": [
    "I can see that mostly all digits are 19 pixels in height, which is comfortable for me. Now I will also examine whether it is the case for test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "79e1dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>filename</th><th>LeftDigitBounds</th><th>RightDigitBounds</th><th>LeftHeight</th><th>LeftWidth</th><th>RightHeight</th><th>RightWidth</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;20000&quot;</td><td>20000.0</td><td>20000.0</td><td>20000.0</td><td>20000.0</td><td>20000.0</td><td>20000.0</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>null</td><td>18.7816</td><td>14.4531</td><td>18.7662</td><td>14.6256</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>0.934208</td><td>3.766445</td><td>0.948094</td><td>3.719707</td></tr><tr><td>&quot;min&quot;</td><td>&quot;./data/test/000158b67254.png&quot;</td><td>null</td><td>null</td><td>9.0</td><td>2.0</td><td>9.0</td><td>2.0</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>null</td><td>19.0</td><td>13.0</td><td>19.0</td><td>13.0</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>null</td><td>19.0</td><td>15.0</td><td>19.0</td><td>15.0</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>null</td><td>19.0</td><td>17.0</td><td>19.0</td><td>18.0</td></tr><tr><td>&quot;max&quot;</td><td>&quot;./data/test/fffc66424609.png&quot;</td><td>null</td><td>null</td><td>19.0</td><td>19.0</td><td>19.0</td><td>19.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 8)\n",
       "┌────────────┬────────────┬────────────┬───────────┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ statistic  ┆ filename   ┆ LeftDigitB ┆ RightDigi ┆ LeftHeigh ┆ LeftWidth ┆ RightHeig ┆ RightWidt │\n",
       "│ ---        ┆ ---        ┆ ounds      ┆ tBounds   ┆ t         ┆ ---       ┆ ht        ┆ h         │\n",
       "│ str        ┆ str        ┆ ---        ┆ ---       ┆ ---       ┆ f64       ┆ ---       ┆ ---       │\n",
       "│            ┆            ┆ f64        ┆ f64       ┆ f64       ┆           ┆ f64       ┆ f64       │\n",
       "╞════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ count      ┆ 20000      ┆ 20000.0    ┆ 20000.0   ┆ 20000.0   ┆ 20000.0   ┆ 20000.0   ┆ 20000.0   │\n",
       "│ null_count ┆ 0          ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
       "│ mean       ┆ null       ┆ null       ┆ null      ┆ 18.7816   ┆ 14.4531   ┆ 18.7662   ┆ 14.6256   │\n",
       "│ std        ┆ null       ┆ null       ┆ null      ┆ 0.934208  ┆ 3.766445  ┆ 0.948094  ┆ 3.719707  │\n",
       "│ min        ┆ ./data/tes ┆ null       ┆ null      ┆ 9.0       ┆ 2.0       ┆ 9.0       ┆ 2.0       │\n",
       "│            ┆ t/000158b6 ┆            ┆           ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 7254.png   ┆            ┆           ┆           ┆           ┆           ┆           │\n",
       "│ 25%        ┆ null       ┆ null       ┆ null      ┆ 19.0      ┆ 13.0      ┆ 19.0      ┆ 13.0      │\n",
       "│ 50%        ┆ null       ┆ null       ┆ null      ┆ 19.0      ┆ 15.0      ┆ 19.0      ┆ 15.0      │\n",
       "│ 75%        ┆ null       ┆ null       ┆ null      ┆ 19.0      ┆ 17.0      ┆ 19.0      ┆ 18.0      │\n",
       "│ max        ┆ ./data/tes ┆ null       ┆ null      ┆ 19.0      ┆ 19.0      ┆ 19.0      ┆ 19.0      │\n",
       "│            ┆ t/fffc6642 ┆            ┆           ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 4609.png   ┆            ┆           ┆           ┆           ┆           ┆           │\n",
       "└────────────┴────────────┴────────────┴───────────┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_preprocessed = preprocess_data_set(test_data)\n",
    "test_data_preprocessed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12aff5f",
   "metadata": {},
   "source": [
    "Okay, seems to be the same situation. Then, on each image's half, I can find first non-zero row and take 19 more rows, and it will be my image. I wonder whether this low dimensionality can make performace worse in comparion to classical mnist with 784 feature encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d387c",
   "metadata": {},
   "source": [
    "# Helping function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4bd5c",
   "metadata": {},
   "source": [
    "Because right parts may have less than 20 pixels in width, pytorch will fail. BTW, I found that MNIST is originaly 20x20, but it was padded to be 28x28 and each digit is centered according to mass center, which makes performance better as model does not have to learn spacial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6f668fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_right_digit(data: pl.DataFrame, idx: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    image_path = data[\"filename\"][idx]\n",
    "    left_, right_ = get_left_right_parts(image_path)\n",
    "    left_start = data[\"LeftDigitBounds\"][idx][0]\n",
    "    right_start = data[\"RightDigitBounds\"][idx][0]\n",
    "\n",
    "    left_col_start = data[\"LeftDigitBounds\"][idx][2]\n",
    "    right_col_start = data[\"RightDigitBounds\"][idx][2]\n",
    "\n",
    "    left_image_trimmed = left_[\n",
    "        left_start : left_start + 19 + 1,\n",
    "        left_col_start : left_col_start + 19 + 1,\n",
    "    ]\n",
    "    right_image_trimmed = right_[\n",
    "        right_start : right_start + 19 + 1,\n",
    "        right_col_start : right_col_start + 19 + 1,\n",
    "    ]\n",
    "\n",
    "    return left_image_trimmed, right_image_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6cdb795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "516a2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_20x20(img: np.ndarray) -> np.ndarray:\n",
    "    h, w = img.shape\n",
    "    pad_h = max(0, 20 - h)\n",
    "    pad_w = max(0, 20 - w)\n",
    "    return np.pad(img, ((0, pad_h), (0, pad_w)), mode=\"constant\", constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b029fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_preprocess(img: np.ndarray) -> np.ndarray:\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    cy, cx = center_of_mass(img)\n",
    "\n",
    "    canvas = np.zeros((28, 28), dtype=np.float32)\n",
    "\n",
    "    top = int(round(14 - cy))\n",
    "    left = int(round(14 - cx))\n",
    "\n",
    "    y1 = max(top, 0)\n",
    "    y2 = min(top + 20, 28)\n",
    "    x1 = max(left, 0)\n",
    "    x2 = min(left + 20, 28)\n",
    "\n",
    "    src_y1 = max(0, -top)\n",
    "    src_y2 = src_y1 + (y2 - y1)\n",
    "    src_x1 = max(0, -left)\n",
    "    src_x2 = src_x1 + (x2 - x1)\n",
    "\n",
    "    canvas[y1:y2, x1:x2] = img[src_y1:src_y2, src_x1:src_x2]\n",
    "\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1dd92c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_deformation(image, alpha=36.0, sigma=5.0):\n",
    "    image_np = np.array(image, dtype=np.float32)\n",
    "\n",
    "    random_state = np.random.RandomState(None)\n",
    "    shape = image_np.shape\n",
    "\n",
    "    dx = (\n",
    "        gaussian_filter(\n",
    "            (random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0\n",
    "        )\n",
    "        * alpha\n",
    "    )\n",
    "    dy = (\n",
    "        gaussian_filter(\n",
    "            (random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0\n",
    "        )\n",
    "        * alpha\n",
    "    )\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
    "    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1))\n",
    "\n",
    "    distorted = map_coordinates(image_np, indices, order=1, mode=\"reflect\").reshape(\n",
    "        shape\n",
    "    )\n",
    "    return Image.fromarray(np.uint8(distorted))\n",
    "\n",
    "\n",
    "def random_affine(image, label, beta=10.0, gamma=20.0):\n",
    "    if label in [1, 7]:\n",
    "        beta = 5.0\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        angle = beta\n",
    "        shear = 0.0\n",
    "    else:\n",
    "        angle = 0.0\n",
    "        shear = beta\n",
    "\n",
    "    affine = T.RandomAffine(\n",
    "        degrees=(-angle, angle),\n",
    "        shear=(-shear, shear),\n",
    "        scale=(1 - gamma / 100, 1 + gamma / 100),\n",
    "        fill=0,\n",
    "    )\n",
    "    return affine(image)\n",
    "\n",
    "\n",
    "def random_augmentation(\n",
    "    img,\n",
    "    class_: int,\n",
    "    alpha_range: tuple[float, float] = (36.0, 38.0),\n",
    "    sigma_range: tuple[float, float] = (5.0, 6.0),\n",
    "    beta: float = 10.0,\n",
    "    gamma_range: tuple[float, float] = (0, 3.0),\n",
    "):\n",
    "    alpha = random.uniform(*alpha_range)\n",
    "    sigma = random.uniform(*sigma_range)\n",
    "    img = elastic_deformation(img, alpha, sigma)\n",
    "    gamma = random.uniform(*gamma_range)\n",
    "    img = random_affine(img, class_, beta, gamma)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ba20dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitDigitDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pl.DataFrame,\n",
    "        only_unique: bool = True,\n",
    "        augment_data: bool = False,\n",
    "        sigma_range: tuple[float, float] = (5.0, 6.0),\n",
    "        alpha_range: tuple[float, float] = (36.0, 38.0),\n",
    "        beta: float = 15.0,\n",
    "        gamma_range: tuple[float, float] = (15.0, 20.0),\n",
    "        mnist_style: bool = False,\n",
    "    ):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.sigma_range = sigma_range\n",
    "        self.alpha_range = alpha_range\n",
    "        self.beta = beta\n",
    "        self.gamma_range = gamma_range\n",
    "\n",
    "        for idx in range(len(data)):\n",
    "            left_digit, right_digit = get_left_right_digit(data, idx)\n",
    "            left_label, right_label = (\n",
    "                int(data[\"label\"][idx][0]),\n",
    "                int(data[\"label\"][idx][1]),\n",
    "            )\n",
    "            left_digit = pad_to_20x20(left_digit)\n",
    "            right_digit = pad_to_20x20(right_digit)\n",
    "\n",
    "            if mnist_style is True:\n",
    "                left_digit = mnist_preprocess(left_digit)\n",
    "                right_digit = mnist_preprocess(right_digit)\n",
    "\n",
    "            self.data.append(torch_transform(left_digit))\n",
    "            self.data.append(torch_transform(right_digit))\n",
    "            self.labels.append(left_label)\n",
    "            self.labels.append(right_label)\n",
    "\n",
    "        self.data = np.array(self.data)\n",
    "        self.labels = np.array(self.labels)\n",
    "        if only_unique is True:\n",
    "            flattened = self.data.reshape(len(self.data), -1)\n",
    "\n",
    "            flattened, idxs = np.unique(flattened, return_index=True, axis=0)\n",
    "\n",
    "            self.data = flattened.reshape(-1, *self.data.shape[1:])\n",
    "            self.labels = self.labels[idxs]\n",
    "\n",
    "        if augment_data:\n",
    "            class_counts = np.bincount(self.labels)\n",
    "            for class_ in range(10):\n",
    "                images_with_class = self.data[self.labels == class_]\n",
    "\n",
    "                new_images = np.max(class_counts) - class_counts[class_] + 5000\n",
    "                if new_images == 0:\n",
    "                    continue\n",
    "\n",
    "                random_mask = np.zeros(class_counts[class_], dtype=bool)\n",
    "                random_mask[0] = True\n",
    "\n",
    "                augmented_images = []\n",
    "                while len(augmented_images) < new_images:\n",
    "                    np.random.shuffle(random_mask)\n",
    "                    img = images_with_class[random_mask]\n",
    "                    alpha = random.uniform(*self.alpha_range)\n",
    "                    sigma = random.uniform(*self.sigma_range)\n",
    "                    img = elastic_deformation(img[0][0], alpha, sigma)\n",
    "                    gamma = random.uniform(*self.gamma_range)\n",
    "                    img = random_affine(img, class_, self.beta, gamma)\n",
    "\n",
    "                    augmented_images.append(torch_transform(img))\n",
    "\n",
    "                self.data = np.concatenate((self.data, np.array(augmented_images)))\n",
    "                self.labels = np.concatenate(\n",
    "                    (self.labels, np.array([class_] * new_images)), dtype=int\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx][0], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec50a9c8",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "I will start implementing MLP with the following  [architecture and parameters](https://github.com/nipunmanral/MLP-Training-For-MNIST-Classification). I will classify each separate digit and than combine them for final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(20 * 20, 350),\n",
    "            nn.BatchNorm1d(350),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(350, 200),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 20 * 20)\n",
    "        feature_vector = self.layers(x)\n",
    "        return self.classifier(feature_vector), feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "de430b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BigMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 2500),\n",
    "            nn.BatchNorm1d(2500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2500, 2000),\n",
    "            nn.BatchNorm1d(2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2000, 1500),\n",
    "            nn.BatchNorm1d(1500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1500, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        feature_vector = self.layers(x)\n",
    "        return self.classifier(feature_vector), feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa359d",
   "metadata": {},
   "source": [
    "Below is code from colab we were given on practise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "01aa9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device=\"cpu\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        _, predicted = output.max(1)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    return running_loss / total, 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "670e5b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data/MNIST/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.MNIST(\"data/MNIST/\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b2a30b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output, _ = model(data)\n",
    "            loss += criterion(output, target).item() * data.size(0)\n",
    "            _, predicted = output.max(1)\n",
    "            predictions.extend(predicted.cpu())\n",
    "            true_values.extend(target.cpu())\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    return (\n",
    "        loss / total,\n",
    "        100.0 * correct / total,\n",
    "        f1_score(true_values, predictions, average=\"macro\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2fbfa287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svc(model, train_loader, device) -> tuple[SVC, float, float]:\n",
    "    model.eval()\n",
    "\n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output, feature_vector = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            feature_vectors.extend(feature_vector.cpu())\n",
    "            labels.extend(target.cpu())\n",
    "\n",
    "    feature_vectors = np.array(feature_vectors)\n",
    "    print(feature_vectors.shape, feature_vectors.dtype)\n",
    "    svm = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\n",
    "                \"svc\",\n",
    "                SVC(kernel=\"rbf\", C=2.0, gamma=\"scale\", class_weight=\"balanced\"),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    svm.fit(feature_vectors, labels)\n",
    "\n",
    "    predictions = svm.predict(feature_vectors)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    macro_f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "\n",
    "    return svm, accuracy, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "9633eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_svc(model, svm, train_loader, device) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "\n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output, feature_vector = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            feature_vectors.extend(feature_vector.cpu())\n",
    "            labels.extend(target.cpu())\n",
    "\n",
    "    feature_vectors = np.array(feature_vectors)\n",
    "    print(feature_vectors.shape, feature_vectors.dtype)\n",
    "\n",
    "    predictions = svm.predict(feature_vectors)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    macro_f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "\n",
    "    return accuracy, macro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d6d23",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd5799",
   "metadata": {},
   "source": [
    "### Training on original data\n",
    "I expect it to perform poorly on original data. Because of high imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365791b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6309.,  6351.,  6340., 63545., 12514.,  6386.,  6239.,  3288.,  3150.,\n",
      "         5878.])\n",
      "tensor([0.9446, 0.9384, 0.9400, 0.0938, 0.4762, 0.9332, 0.9552, 1.8126, 1.8920,\n",
      "        1.0139])\n",
      "Epoch 1: Train loss=0.3269, acc=89.81% | Test loss=0.1524, acc=95.62%, macroF1=0.92\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "train_dataset_full = SplitDigitDataset(train_data_preprocessed, only_unique=False)\n",
    "\n",
    "model = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loader = DataLoader(train_dataset_full, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(train_dataset_full, batch_size=1000, shuffle=False)\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc, macro_f1 = evaluate(model, test_loader, criterion, device)\n",
    "    print(\n",
    "        f\"Epoch {epoch}: \"\n",
    "        f\"Train loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n",
    "        f\"Test loss={test_loss:.4f}, acc={test_acc:.2f}%, macroF1={macro_f1:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20acc5f5",
   "metadata": {},
   "source": [
    "---\n",
    "### Adding data augmentation\n",
    "I will also apply some data augmentations as documented in [this paper](https://arxiv.org/pdf/1003.0358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab140d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=0.4362, acc=85.95% | Test loss=0.2340, acc=92.34%, macroF1=0.92\n",
      "Epoch 2: Train loss=0.2315, acc=92.75% | Test loss=0.2011, acc=93.65%, macroF1=0.94\n",
      "Epoch 3: Train loss=0.1777, acc=94.39% | Test loss=0.1408, acc=95.36%, macroF1=0.95\n",
      "Epoch 4: Train loss=0.1497, acc=95.18% | Test loss=0.1195, acc=96.11%, macroF1=0.96\n",
      "Epoch 5: Train loss=0.1243, acc=95.96% | Test loss=0.1066, acc=96.44%, macroF1=0.96\n",
      "Epoch 6: Train loss=0.1089, acc=96.46% | Test loss=0.0925, acc=97.00%, macroF1=0.97\n",
      "Epoch 7: Train loss=0.0977, acc=96.83% | Test loss=0.0638, acc=97.93%, macroF1=0.98\n",
      "Epoch 8: Train loss=0.0886, acc=97.15% | Test loss=0.0760, acc=97.50%, macroF1=0.98\n",
      "Epoch 9: Train loss=0.0822, acc=97.31% | Test loss=0.0673, acc=97.78%, macroF1=0.98\n",
      "Epoch 10: Train loss=0.0730, acc=97.61% | Test loss=0.0659, acc=97.74%, macroF1=0.98\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "model = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_dataset_full = SplitDigitDataset(\n",
    "    train_data_preprocessed, only_unique=True, augment_data=True\n",
    ")\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loader = DataLoader(train_dataset_full, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(train_dataset_full, batch_size=1000, shuffle=False)\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc, macro_f1 = evaluate(model, test_loader, criterion, device)\n",
    "    print(\n",
    "        f\"Epoch {epoch}: \"\n",
    "        f\"Train loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n",
    "        f\"Test loss={test_loss:.4f}, acc={test_acc:.2f}%, macroF1={macro_f1:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1143d3d",
   "metadata": {},
   "source": [
    "---\n",
    "### Augmented data + SVM\n",
    "This is one of the solutions that yielded one of the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4e85d81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=0.7312, acc=77.10% | Test loss=0.3654, acc=88.42%, macroF1=0.88\n",
      "Epoch 2: Train loss=0.4794, acc=84.75% | Test loss=0.3173, acc=89.75%, macroF1=0.90\n",
      "Epoch 3: Train loss=0.4170, acc=86.74% | Test loss=0.2890, acc=90.60%, macroF1=0.91\n",
      "Epoch 4: Train loss=0.3759, acc=87.86% | Test loss=0.2754, acc=91.22%, macroF1=0.91\n",
      "Epoch 5: Train loss=0.3483, acc=88.83% | Test loss=0.2553, acc=91.83%, macroF1=0.92\n",
      "(86736, 100) float32\n",
      "(21684, 100) float32\n",
      "Train SVM Accuracy 0.9584486257148128\n",
      "Train SVM F1 Macro 0.9584189417238426\n",
      "Test SVM Accuracy 0.9307323372071573\n",
      "Test SVM F1 Macro 0.9310004147869536\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "model_svm = MLP().to(device)\n",
    "criterion_svm = nn.CrossEntropyLoss()\n",
    "optimizer_svm = optim.Adam(model_svm.parameters(), lr=1e-3)\n",
    "\n",
    "train_dataset_full = SplitDigitDataset(\n",
    "    train_data_preprocessed, only_unique=True, augment_data=True\n",
    ")\n",
    "\n",
    "full_size = len(train_dataset_full)\n",
    "train_size = int(0.8 * full_size)\n",
    "test_size = full_size - train_size\n",
    "\n",
    "train_data, test_data = random_split(train_dataset_full, (train_size, test_size))\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "    train_loss, train_acc = train(\n",
    "        model_svm, train_loader, criterion_svm, optimizer_svm, device\n",
    "    )\n",
    "    test_loss, test_acc, macro_f1 = evaluate(\n",
    "        model_svm, test_loader, criterion_svm, device\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch}: \"\n",
    "        f\"Train loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n",
    "        f\"Test loss={test_loss:.4f}, acc={test_acc:.2f}%, macroF1={macro_f1:.2f}\"\n",
    "    )\n",
    "\n",
    "svm, acc_train, f1_train = train_svc(\n",
    "    model_svm, DataLoader(train_data, batch_size=5000, shuffle=False), device\n",
    ")\n",
    "\n",
    "acc_test, f1_test = evaluate_svc(\n",
    "    model_svm, svm, DataLoader(test_data, batch_size=5000, shuffle=False), device\n",
    ")\n",
    "print(\"Train SVM Accuracy\", acc_train)\n",
    "print(\"Train SVM F1 Macro\", f1_train)\n",
    "print(\"Test SVM Accuracy\", acc_test)\n",
    "print(\"Test SVM F1 Macro\", f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e426f",
   "metadata": {},
   "source": [
    "### Original MNIST.\n",
    "Original MNIST has better data balance, so I will suffer less overfit. I will also have to preprocess cropped digits to look like in MNIST. (i.e. add black border paddings, center by center of mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e8354f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=0.3845, acc=88.21% | Test loss=0.2214, acc=94.28%, macroF1=0.94\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "mnist_model = MLP().to(device)\n",
    "mnist_criterion = nn.CrossEntropyLoss()\n",
    "mnist_optimizer = optim.Adam(mnist_model.parameters(), lr=1e-3)\n",
    "\n",
    "test_dataset = SplitDigitDataset(train_data_preprocessed)\n",
    "mnist_dataset = datasets.MNIST(\n",
    "    root=\"data/MNIST/\",\n",
    "    train=True,\n",
    "    transform=torch_transform,\n",
    ")\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loader = DataLoader(mnist_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "    train_loss, train_acc = train(\n",
    "        mnist_model, train_loader, mnist_criterion, mnist_optimizer, device\n",
    "    )\n",
    "    test_loss, test_acc, macro_f1 = evaluate(\n",
    "        mnist_model, test_loader, mnist_criterion, device\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch}: \"\n",
    "        f\"Train loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n",
    "        f\"Test loss={test_loss:.4f}, acc={test_acc:.2f}%, macroF1={macro_f1:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ecf532",
   "metadata": {},
   "source": [
    "---\n",
    "### Original mnist with data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "50d9f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformedMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root=\"./data/MNIST\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        sigma_range=(5.0, 6.0),\n",
    "        alpha_range=(36.0, 38.0),\n",
    "        beta=15.0,\n",
    "        gamma_range=(15.0, 20.0),\n",
    "    ):\n",
    "        self.mnist = torchvision.datasets.MNIST(\n",
    "            root=root, train=train, download=download\n",
    "        )\n",
    "        self.sigma_range = sigma_range\n",
    "        self.alpha_range = alpha_range\n",
    "        self.beta = beta\n",
    "        self.gamma_range = gamma_range\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist[idx]\n",
    "\n",
    "        sigma = random.uniform(*self.sigma_range)\n",
    "        alpha = random.uniform(*self.alpha_range)\n",
    "        gamma = random.uniform(*self.gamma_range)\n",
    "\n",
    "        image = elastic_deformation(image, alpha, sigma)\n",
    "        image = random_affine(image, label, self.beta, gamma)\n",
    "\n",
    "        return torch_transform(image), label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59feeac1",
   "metadata": {},
   "source": [
    "---\n",
    "### Original MNIST dataset with support vector machine for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9fb1473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=0.5068, acc=84.07% | Test loss=0.1242, acc=96.04%, macroF1=0.96\n",
      "Epoch 2: Train loss=0.3020, acc=90.39% | Test loss=0.0672, acc=98.02%, macroF1=0.98\n",
      "Epoch 3: Train loss=0.2568, acc=91.86% | Test loss=0.0812, acc=97.45%, macroF1=0.97\n",
      "Epoch 4: Train loss=0.2278, acc=92.75% | Test loss=0.0538, acc=98.32%, macroF1=0.98\n",
      "Epoch 5: Train loss=0.2063, acc=93.50% | Test loss=0.0520, acc=98.36%, macroF1=0.98\n",
      "(60000, 100) float32\n",
      "Accuracy 0.9632333333333334\n",
      "F1 Macro 0.9628488224546917\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "mnist_svc_model = BigMLP().to(device)\n",
    "mnist_svc_criterion = nn.CrossEntropyLoss()\n",
    "mnist_svc_optimizer = optim.Adam(mnist_svc_model.parameters(), lr=1e-3)\n",
    "\n",
    "test_dataset = SplitDigitDataset(\n",
    "    train_data_preprocessed,\n",
    "    only_unique=True,\n",
    "    augment_data=False,\n",
    "    mnist_style=True,\n",
    ")\n",
    "mnist_aug_dataset = DeformedMNIST()\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loader = DataLoader(mnist_aug_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "    train_loss, train_acc = train(\n",
    "        mnist_svc_model, train_loader, mnist_svc_criterion, mnist_svc_optimizer, device\n",
    "    )\n",
    "    test_loss, test_acc, macro_f1 = evaluate(\n",
    "        mnist_svc_model, test_loader, mnist_svc_criterion, device\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch}: \"\n",
    "        f\"Train loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n",
    "        f\"Test loss={test_loss:.4f}, acc={test_acc:.2f}%, macroF1={macro_f1:.2f}\"\n",
    "    )\n",
    "\n",
    "svm_mnist, acc, f1 = train_svc(\n",
    "    mnist_svc_model,\n",
    "    DataLoader(mnist_aug_dataset, batch_size=5000, shuffle=False),\n",
    "    device,\n",
    ")\n",
    "print(\"Accuracy\", acc)\n",
    "print(\"F1 Macro\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f9b12",
   "metadata": {},
   "source": [
    "In the end, my \"best\" solutions require a lot of epoch to learn all the augmentations. Those solutions are not hopeless, but there is a way to go still.\n",
    "\n",
    "I considered doing test time augmentation, but it drastically descreased performance to macro f1 of 0.1 :(\n",
    "I also considered create 5-6 MLP's with different architectures and feeding them with different augmentations of the same sample, and then do soft voting, but I did not get enough time for that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2113b",
   "metadata": {},
   "source": [
    "# Compiling submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "0145cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dataset = SplitDigitDataset(\n",
    "    test_data_preprocessed.with_columns(pl.lit(\"00\").alias(\"label\")),\n",
    "    only_unique=False,\n",
    "    mnist_style=True,\n",
    ")\n",
    "\n",
    "submission_loader = DataLoader(submission_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26b13f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission(model, test_loader, test_data_raw: pl.DataFrame) -> pl.DataFrame:\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, _ = model(data)\n",
    "            _, predicted = output.max(axis=1)\n",
    "            outputs.extend(predicted.cpu())\n",
    "\n",
    "    outputs = np.apply_along_axis(\n",
    "        lambda digits: int(str(digits[0]) + str(digits[1])),\n",
    "        axis=1,\n",
    "        arr=np.array(outputs).reshape(-1, 2),\n",
    "    )\n",
    "\n",
    "    return pl.concat(\n",
    "        [test_data_raw, pl.from_numpy(outputs, schema=[\"label\"])], how=\"horizontal\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b9130c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = get_submission(model, submission_loader, pl.read_csv(\"./data/test.csv\"))\n",
    "submission.write_csv(\"./data/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8f29274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_mnist = get_submission(\n",
    "    mnist_model, submission_loader, pl.read_csv(\"./data/test.csv\")\n",
    ")\n",
    "submission_mnist.write_csv(\"./data/submission_mnist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4dedfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_aug_mnist = get_submission(\n",
    "    mnist_aug_model, submission_loader, pl.read_csv(\"./data/test.csv\")\n",
    ")\n",
    "submission_aug_mnist.write_csv(\"./data/submission_aug_mnist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "476777db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission_with_svm(\n",
    "    model, svm, test_loader, test_data_raw: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    feature_vectors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            _, feature_vector = model(data)\n",
    "            feature_vectors.extend(feature_vector.cpu())\n",
    "\n",
    "    outputs = svm.predict(feature_vectors)\n",
    "\n",
    "    outputs = np.apply_along_axis(\n",
    "        lambda digits: int(str(digits[0]) + str(digits[1])),\n",
    "        axis=1,\n",
    "        arr=np.array(outputs).reshape(-1, 2),\n",
    "    )\n",
    "\n",
    "    return pl.concat(\n",
    "        [test_data_raw, pl.from_numpy(outputs, schema=[\"label\"])], how=\"horizontal\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "51619322",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_svm = get_submission_with_svm(\n",
    "    model_svm, svm, submission_loader, pl.read_csv(\"./data/test.csv\")\n",
    ")\n",
    "submission_svm.write_csv(\"./data/submission_svm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a2b52358",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_svm_mnist = get_submission_with_svm(\n",
    "    mnist_svc_model, svm_mnist, submission_loader, pl.read_csv(\"./data/test.csv\")\n",
    ")\n",
    "submission_svm_mnist.write_csv(\"./data/submission_svm_mnist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d9710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
